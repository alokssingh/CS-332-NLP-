{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "normalisation-lemmatization-stemming.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pfSRWOfaHIGucNtpZ-_n465YbsmDPflO",
      "authorship_tag": "ABX9TyMoZ8PdtL7vc51TTg+Us+lD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alokssingh/CS-332-NLP-Tutoiral-Notes/blob/main/Tutorial%201/normalisation_lemmatization_stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text normalisation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   **Process of transforming text into a single canonical form**\n",
        "2.   **Before text normalization we should be aware of**\n",
        "    \n",
        "       **i)** what type of text is to be normalized and,\n",
        "      \n",
        "      **ii)** how it is to be processed afterwards\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1r0q5LwQ2aDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why text normalisation ?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> 1) **In string searching**   *e.g. ‘john’ and ‘John’ (Case based matching)*\n",
        "\n",
        "> 2) **American or British English spelling**\n",
        "\n",
        "> 3) **Multiple form of single word** e.g USA or US\n",
        "\n",
        "> 4) **frequently used in converting text to speech**\n",
        "\n",
        "> > Numbers, dates, acronyms, and abbreviations are non-standard \"words\" that need to be pronounced differently depending on context\n"
      ],
      "metadata": {
        "id": "QvYAJtx63jCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample example of text normalisation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "T7HRWxPXQQVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "b2m_-n035R5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = ['The Patient! is s wai@ting, for5 you in room Number','johN', 'John', 'JOHN']\n",
        "print(data_list)"
      ],
      "metadata": {
        "id": "wZZNj-aH7_-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = []"
      ],
      "metadata": {
        "id": "mDfMBFG4PRF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(data_list)):\n",
        "\t\tdata = data_list[i]\n",
        "\t\t# Tokenize i.e. split on white spaces\n",
        "\t\tdata = data.split()\n",
        "\t\t# Convert to lowercase\n",
        "\t\tdata = [word.lower() for word in data]\n",
        "  \t# Remove punctuation from each token\n",
        "\t  # Prepare translation table for removing punctuation\n",
        "\t\ttable = str.maketrans('', '', string.punctuation)    \n",
        "\t\tdata = [w.translate(table) for w in data]\n",
        "\t\t#Remove hanging 's' and 'a'\n",
        "\t\t#data = [word for word in data if len(word)>1]\n",
        "\t\t# Remove tokens with special character\n",
        "\t\tdata = [re.sub(r\"[@?\\(^)+\\) 0-9]\", \"\", word) for word in data ]  # Note: re.sub(pattern, repl, string) \n",
        "\t\t# Store as string\n",
        "\t\tdata  =  ' '.join(data)\n",
        "\t\tpreprocessed_text.append(data)"
      ],
      "metadata": {
        "id": "uNuO-RtO5SEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"******Text before preprocessing******\")\n",
        "for text in data_list:\n",
        "  print(text)\n",
        "\n",
        "print(\"******Text after preprocessing******\")\n",
        "for text in preprocessed_text:\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "baqQU1SC5SHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Parsing**\n",
        "\n",
        "---\n",
        "\n",
        "# **2. Morpheme** \n",
        "\n",
        "---\n",
        "# **3. Stemming** \n",
        "\n",
        "  *   chopping affies from a word \n",
        "  *   may or may not has dictionary meaning  \n",
        "\n",
        "* Used in **information retrieval** for searching, Sentiment Analysis, document clustring. e.g search for party (search engine will show parties)  \n",
        "\n",
        "* Mostly used stemmer **Porter stemmer**\n",
        "* other stemmers: \n",
        "\n",
        "        1. Lovins Stemmer \n",
        "\n",
        "        2. Dawson Stemmer\n",
        "\n",
        "        3. Krovetz Stemmer\n",
        "\n",
        "        4. Xerox Stemmer \n",
        "\n",
        "        5. N-Gram Stemmer \n",
        "\n",
        "        6. Snowball Stemmer \n",
        "\n",
        "        7. Lancaster stemmers\n",
        "\n",
        "---\n",
        "[https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.htmlt](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
      ],
      "metadata": {
        "id": "A0y1PntIRcm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to use Porter stemmer\n",
        "\n",
        "**Step 1** - Import the NLTK library and from NLTK import PorterStemmer\n",
        "\n",
        "**Step 2** - Creat a variable and store PorterStemmer into it\n",
        "\n",
        "**Step 3** - use PorterStemmer\n"
      ],
      "metadata": {
        "id": "_4M1PE9o2Fe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming of words"
      ],
      "metadata": {
        "id": "RW4HTbIt5kOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "VpGBezlK2EE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "sZsD6pSL2ikS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem('bat'))\n",
        "print(ps.stem('batting'))"
      ],
      "metadata": {
        "id": "re6h-zOh2mZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming of a sentence"
      ],
      "metadata": {
        "id": "4hxfzfpm3pvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "0LsPzc813t02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was not the map we found in Billy Bones’s chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "6Qb86Hf72qht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text) # or you can use above approach of spliting based on space and cleaning the text like normalization\n",
        "print(words)"
      ],
      "metadata": {
        "id": "kvzcjkxC345D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemed_words = []\n",
        "for w in words:\n",
        "  stemed_words.append(ps.stem(w)) "
      ],
      "metadata": {
        "id": "fGgwPcQQ3InH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemed_words)"
      ],
      "metadata": {
        "id": "f42c5yfQ4uIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list into text stream\n",
        "stemmed_text = ' '.join(stemed_words)\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "id": "U3hsnosN5BdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Lemmatization**\n",
        "\n",
        "---\n",
        "\n",
        "1. task of determining that two words have the same root, despite their surface differences\n",
        "2. **Studies and Studying** ---> *Study* \n",
        "3. have dictionary meaning\n",
        "4. e.g lemmatized form of a sentence:\n",
        "     \n",
        "\n",
        "       He is reading detective stories\n",
        "       ⏬ ⏬  ⏬         ⏬        ⏬\n",
        "       He be read   detective   story     NOTE:  am, are, and is have the shared lemma be\n",
        "\n",
        "5. Different lemmatizers:\n",
        "\n",
        "        1. WordnetLemmatizer\n",
        "\n",
        "        2. spaCy\n",
        "\n",
        "        3. TexxtBlob Lemmatizer\n",
        "\n",
        "        4. Pattern Lemmatizer\n",
        "\n",
        "        5. Standford CoreNLP Lemmatizer\n",
        "\n",
        "        6. Gensim lemmatizer            "
      ],
      "metadata": {
        "id": "MeHq5AJd2AU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization of words"
      ],
      "metadata": {
        "id": "dp_BPCRA6U3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') #wordnet last tutorial \n",
        "nltk.download('averaged_perceptron_tagger') # will download pos tag "
      ],
      "metadata": {
        "id": "AVp1w3cO4tDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "q73xv7uh5SWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('sang'))\n",
        "print(lemmatizer.lemmatize('sung'))\n",
        "print(lemmatizer.lemmatize('sings'))"
      ],
      "metadata": {
        "id": "XT4DPyu25SYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS)*\n",
        "\n",
        "**Sometimes, the same word can have a multiple lemmas based on the meaning / context.**"
      ],
      "metadata": {
        "id": "FeJvhRUZAZUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.pos_tag('sang'))\n",
        "print(nltk.pos_tag(['sing']))"
      ],
      "metadata": {
        "id": "R6j0Wucq89_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('sang',pos='v'))\n",
        "print(lemmatizer.lemmatize('sung',pos='v'))\n",
        "print(lemmatizer.lemmatize('sings',pos='v'))"
      ],
      "metadata": {
        "id": "xy8BNOF69jAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization of text"
      ],
      "metadata": {
        "id": "shYeaF5B6lDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was not the map we found in Billy Bones’s chest, but an accurate copy, complete in all things names and heights and soundings with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "VhAE_Yh15Sbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text) # or you can use above approach of spliting based on space and cleaning the text like normalization\n",
        "print(words)"
      ],
      "metadata": {
        "id": "xYy-4RgF5Sfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_words = []\n",
        "for w in words:\n",
        "  lemma_words.append(lemmatizer.lemmatize(w, pos='v')) "
      ],
      "metadata": {
        "id": "grkfkEaG5SiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNesQuOnD_Vs"
      },
      "outputs": [],
      "source": [
        "print(lemma_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list into text stream\n",
        "print(\"***** Text before lemmatization*****\\n\")\n",
        "print(text)\n",
        "lemmatized_text = ' '.join(lemma_words)\n",
        "print(\"\\n***** Text after lemmatization*****\\n\")\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "id": "ts8rbkY72ZH2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}