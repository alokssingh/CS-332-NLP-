{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "normalisation-lemmatization-stemming.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SHaxZKQV_wvU9RcFnPA3XLieEEYmqD-M",
      "authorship_tag": "ABX9TyM12YXDq00yeRU//O7dzOv7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alokssingh/CS-332-NLP-Tutorials-Notes/blob/main/Tutorial%202/normalisation_lemmatization_stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  **Tutorial 2**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  **Normalization**\n",
        "*   **Parsing**\n",
        "*   **Morpheme**\n",
        "*   **Stemming**\n",
        "*   **Lemmatization**\n"
      ],
      "metadata": {
        "id": "h2ensMy5RGbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text normalisation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   **Process of transforming text into a single canonical form**\n",
        "2.   **Before text normalization we should be aware of**\n",
        "    \n",
        "       **i)** what type of text is to be normalized and,\n",
        "      \n",
        "      **ii)** how it is to be processed afterwards\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1r0q5LwQ2aDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why text normalisation ?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> 1) **In string searching**   *e.g. ‘john’ and ‘John’ (Case based matching)*\n",
        "\n",
        "> 2) **American or British English spelling**\n",
        "\n",
        "> 3) **Multiple form of single word** e.g USA or US\n",
        "\n",
        "> 4) **frequently used in converting text to speech**\n",
        "\n",
        "> > Numbers, dates, acronyms, and abbreviations are non-standard \"words\" that need to be pronounced differently depending on context\n"
      ],
      "metadata": {
        "id": "QvYAJtx63jCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample example of text normalisation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "T7HRWxPXQQVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "b2m_-n035R5K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = ['The Patient! is s wai@ting, for5 you in room Number'] #\n",
        "print(data_list[0])"
      ],
      "metadata": {
        "id": "wZZNj-aH7_-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a954d0-b51f-428b-a40c-ddf54443b643"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Patient! is s wai@ting, for5 you in room Number\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = []"
      ],
      "metadata": {
        "id": "mDfMBFG4PRF_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data_list)):\n",
        "\t\tdata = data_list[i]\n",
        "\t\t# Tokenize i.e. split on white spaces\n",
        "\t\tdata = data.split()\n",
        "\t\t# Convert to lowercase\n",
        "\t\tdata = [word.lower() for word in data]\n",
        "  \t# Remove punctuation from each token\n",
        "\t  # Prepare translation table for removing punctuation\n",
        "\t\ttable = str.maketrans('', '', string.punctuation)    \n",
        "\t\tdata = [w.translate(table) for w in data]\n",
        "\t\t#Remove hanging 's' and 'a'\n",
        "\t\t#data = [word for word in data if len(word)>1]\n",
        "\t\t# Remove tokens with special character\n",
        "\t\tdata = [re.sub(r\"[@?\\(^)+\\) 0-9]\", \"\", word) for word in data ]  # Note: re.sub(pattern, repl, string) \n",
        "\t\t# Store as string\n",
        "\t\tdata  =  ' '.join(data)\n",
        "\t\tpreprocessed_text.append(data)"
      ],
      "metadata": {
        "id": "uNuO-RtO5SEF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"******Text before preprocessing******\")\n",
        "for text in data_list:\n",
        "  print(text)\n",
        "\n",
        "print(\"******Text after preprocessing******\")\n",
        "for text in preprocessed_text:\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "baqQU1SC5SHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cac1fdc-05d4-4ecb-9d7e-328a5195b88d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******Text before preprocessing******\n",
            "The Patient! is s wai@ting, for5 you in room Number\n",
            "johN\n",
            "John\n",
            "JOHN\n",
            "******Text after preprocessing******\n",
            "the patient is s waiting for you in room number\n",
            "john\n",
            "john\n",
            "john\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cat s stem and affix "
      ],
      "metadata": {
        "id": "qD8zrWY-m7Pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Parsing**\n",
        "\n",
        "---\n",
        "\n",
        "# **2. Morpheme** \n",
        "\n",
        "---\n",
        "# **3. Stemming** \n",
        "\n",
        "  *   chopping affies from a word \n",
        "  *   may or may not has dictionary meaning  \n",
        "\n",
        "* Used in **information retrieval** for searching, **Sentiment Analysis**, **document clustring**. e.g search for party (search engine will show parties)  \n",
        "\n",
        "* Mostly used stemmer **Porter stemmer**\n",
        "* other stemmers: \n",
        "\n",
        "        1. Lovins Stemmer \n",
        "\n",
        "        2. Dawson Stemmer\n",
        "\n",
        "        3. Krovetz Stemmer\n",
        "\n",
        "        4. Xerox Stemmer \n",
        "\n",
        "        5. N-Gram Stemmer \n",
        "\n",
        "        6. Snowball Stemmer \n",
        "\n",
        "        7. Lancaster stemmers\n",
        "\n",
        "---\n",
        "[https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.htmlt](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
      ],
      "metadata": {
        "id": "A0y1PntIRcm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to use Porter stemmer\n",
        "\n",
        "**Step 1** - Import the NLTK library and from NLTK import PorterStemmer\n",
        "\n",
        "**Step 2** - Creat a variable and store PorterStemmer into it\n",
        "\n",
        "**Step 3** - use PorterStemmer\n"
      ],
      "metadata": {
        "id": "_4M1PE9o2Fe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming of words"
      ],
      "metadata": {
        "id": "RW4HTbIt5kOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "VpGBezlK2EE_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "sZsD6pSL2ikS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem('foxes'))\n",
        "print(ps.stem('Studying'))"
      ],
      "metadata": {
        "id": "re6h-zOh2mZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667b34da-c68c-429a-9011-1d82e18a7619"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studi\n",
            "studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming of a sentence"
      ],
      "metadata": {
        "id": "4hxfzfpm3pvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "0LsPzc813t02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d87091f-3069-4389-fbdb-adabeae7e08e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was not the map we found in Billy Bones’s chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "6Qb86Hf72qht"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text) # or you can use above approach of spliting based on space and cleaning the text like normalization\n",
        "print(words)"
      ],
      "metadata": {
        "id": "kvzcjkxC345D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f202030-3cd7-47f9-b96a-cd189d9c845e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', '’', 's', 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things-names', 'and', 'heights', 'and', 'soundings-with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemed_words = []\n",
        "for w in words:\n",
        "  stemed_words.append(ps.stem(w)) "
      ],
      "metadata": {
        "id": "fGgwPcQQ3InH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemed_words)"
      ],
      "metadata": {
        "id": "f42c5yfQ4uIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7164c161-6d98-444f-d2dd-15e40304a1f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', '’', 's', 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'things-nam', 'and', 'height', 'and', 'soundings-with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list into text stream\n",
        "stemmed_text = ' '.join(stemed_words)\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "id": "U3hsnosN5BdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6759840-ae78-4ce0-963f-04398d1848c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thi wa not the map we found in billi bone ’ s chest , but an accur copi , complet in all things-nam and height and soundings-with the singl except of the red cross and the written note .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Lemmatization**\n",
        "\n",
        "---\n",
        "\n",
        "1. task of determining that two words have the same root, despite their surface differences\n",
        "2. **Studies and Studying** ---> *Study* \n",
        "3. have dictionary meaning\n",
        "4. e.g lemmatized form of a sentence:\n",
        "     \n",
        "\n",
        "       He is reading detective stories\n",
        "       ⏬ ⏬  ⏬         ⏬        ⏬\n",
        "       He be read   detective   story     NOTE:  am, are, and is have the shared lemma be\n",
        "\n",
        "5. Different lemmatizers:\n",
        "\n",
        "        1. WordnetLemmatizer\n",
        "\n",
        "        2. spaCy\n",
        "\n",
        "        3. TexxtBlob Lemmatizer\n",
        "\n",
        "        4. Pattern Lemmatizer\n",
        "\n",
        "        5. Standford CoreNLP Lemmatizer\n",
        "\n",
        "        6. Gensim lemmatizer            "
      ],
      "metadata": {
        "id": "MeHq5AJd2AU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization of words"
      ],
      "metadata": {
        "id": "dp_BPCRA6U3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') #wordnet last tutorial \n",
        "nltk.download('averaged_perceptron_tagger') # will download pos tag "
      ],
      "metadata": {
        "id": "AVp1w3cO4tDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa2f0bd-2716-4520-a68f-9dd5299c66f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "q73xv7uh5SWI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('sang'))\n",
        "print(lemmatizer.lemmatize('sung'))\n",
        "print(lemmatizer.lemmatize('sings'))"
      ],
      "metadata": {
        "id": "XT4DPyu25SYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa458d1-a837-46da-cd73-bcf340dc703b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sang\n",
            "sung\n",
            "sings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS)*\n",
        "\n",
        "**Sometimes, the same word can have a multiple lemmas based on the meaning / context.**"
      ],
      "metadata": {
        "id": "FeJvhRUZAZUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.pos_tag('sang'))\n",
        "print(nltk.pos_tag(['sing']))"
      ],
      "metadata": {
        "id": "R6j0Wucq89_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6345a50d-f588-4ff6-eaed-739bf8dda45e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('s', 'VB'), ('a', 'DT'), ('n', 'JJ'), ('g', 'NN')]\n",
            "[('sing', 'VBG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('sang',pos='v'))\n",
        "print(lemmatizer.lemmatize('sung',pos='v'))\n",
        "print(lemmatizer.lemmatize('sings',pos='v'))"
      ],
      "metadata": {
        "id": "xy8BNOF69jAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d636a4a9-139c-4906-be9f-3bac832343af"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sing\n",
            "sing\n",
            "sing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization of text"
      ],
      "metadata": {
        "id": "shYeaF5B6lDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was not the map we found in Billy Bones’s chest, @ but an accurate copy, complete in all things names and heights and soundings with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "VhAE_Yh15Sbi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.split('@')\n",
        "print(text)"
      ],
      "metadata": {
        "id": "1X4w3ae3qYKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text) # or you can use above approach of spliting based on space and cleaning the text like normalization\n",
        "print(words)"
      ],
      "metadata": {
        "id": "xYy-4RgF5Sfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0020e8-bd14-414d-ea7a-1f8b58be7045"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', '’', 's', 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', 'names', 'and', 'heights', 'and', 'soundings', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_words = []\n",
        "for w in words:\n",
        "  lemma_words.append(lemmatizer.lemmatize(w, pos='v')) "
      ],
      "metadata": {
        "id": "grkfkEaG5SiW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hNesQuOnD_Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b51402-d2ba-44df-c53f-eb44abf50016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'be', 'not', 'the', 'map', 'we', 'find', 'in', 'Billy', 'Bones', '’', 's', 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', 'name', 'and', 'heights', 'and', 'sound', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'cross', 'and', 'the', 'write', 'note', '.']\n"
          ]
        }
      ],
      "source": [
        "print(lemma_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list into text stream\n",
        "print(\"***** Text before lemmatization*****\\n\")\n",
        "print(text)\n",
        "lemmatized_text = ' '.join(lemma_words)\n",
        "print(\"\\n***** Text after lemmatization*****\\n\")\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "id": "ts8rbkY72ZH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f1f348-41aa-44f6-b144-eb16865d11ee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Text before lemmatization*****\n",
            "\n",
            "This was not the map we found in Billy Bones’s chest, but an accurate copy, complete in all things names and heights and soundings with the single exception of the red crosses and the written notes.\n",
            "\n",
            "***** Text after lemmatization*****\n",
            "\n",
            "This be not the map we find in Billy Bones ’ s chest , but an accurate copy , complete in all things name and heights and sound with the single exception of the red cross and the write note .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "text = \"The study of grammar has an ancie5nt pe6digree; Panini’s grammar of Sanskrit was written over two thousand ye@ars ago and is st\\ill referenced today in teaching San]skrit. Despite this histor$y, knowledge of grammar remains spotty at best. In this chapter, we make a pr%eliminary st-ab at addressing some of these gaps in our knowledge of grammar and syntax, (as well as introducing some of the formal mechanisms that are available for capturing this knowledge in a computationally useful manner.\"\n",
        "\n",
        "1. Perform normalization on above text and then do below actions:\n",
        "\n",
        "       Apply all the stemmers to the above text and note down the pro and cons of each stemmer\n",
        "  \n",
        "       Use all the lemmatizer on about text and note down the pro and cons of each lemmatizer"
      ],
      "metadata": {
        "id": "jWjJQR_fH4W_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "text2 = \"व्याकरण के अध्ययन में एक पुरानी 5 वीं डिग्री है; पाणिनी का संस्कृत व्याकरण दो हजार साल पहले लिखा गया था और आज भी संस्कृत के संस्कृत शिक्षण में इसका उल्लेख नहीं किया गया है। इस इतिहास के बावजूद, व्याकरण का ज्ञान सबसे अच्छा रहता है। इस अध्याय में, हम व्याकरण और वाक्य रचना के अपने ज्ञान में इनमें से कुछ अंतरालों को संबोधित करने के लिए एक pr% एलिमिनरी st-ab बनाते हैं, (साथ ही कुछ औपचारिक तंत्रों को पेश करते हैं जो इस ज्ञान को कम्प्यूटेशनल रूप से उपयोगी तरीके से कैप्चर करने के लिए उपलब्ध हैं।\"\n",
        "\n",
        "\n",
        "1. Perform normalization on above text and then do below actions:\n",
        "\n",
        "       Perform stemming to the above text using IndicNLP tokenizer\n",
        "  \n",
        "       Perform lemmatizing on about text using IndicNLP tokenizer\n",
        "\n",
        "For IndicNLP ⏬⏬⏬\n",
        "\n",
        "[IndicNLP](https://anoopkunchukuttan.github.io/indic_nlp_library/)\n",
        "\n",
        "[IndicNLP site](https://indicnlp.org/)"
      ],
      "metadata": {
        "id": "EWW0ktG4vpxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hints\n",
        "\n",
        "**Libraries required for other lemmatizers**\n",
        "\n",
        "        import spacy\n",
        "\n",
        "        from textblob import TextBlob, Word\n",
        "\n",
        "        import pattern (colab !pip instal pattern)\n",
        "\n",
        "        from stanfordcorenlp import StanfordCoreNLP\n",
        "\n",
        "        from gensim.utils import lemmatize\n",
        "\n",
        "**Libraries required for other stemmers**\n",
        "\n",
        "        from nltk.stem.snowball import SnowballStemmer\n"
      ],
      "metadata": {
        "id": "QEnfGLndShIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. http://www.nltk.org/install.html\n",
        "\n",
        "2. [https://text-processing.com/demo/stem/](https://text-processing.com/demo/stem/)\n",
        "\n",
        "3. [http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf](http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf)\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lXZ5gprcULdF"
      }
    }
  ]
}